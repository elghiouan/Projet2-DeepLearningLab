{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmVU7UX02te"
      },
      "source": [
        "# Projet : Exploration RAG et RLHF\n",
        "\n",
        "**Binôme 7** :\n",
        "**EL GHIOUAN Israe & BENCHERAIK ABDESSAMAD**\n",
        "\n",
        "Lien Github : https://github.com/elghiouan/Projet2-DeepLearningLab/\n",
        "\n",
        "Ce notebook démontre :\n",
        "1.  **Génération Augmentée par Récupération (RAG) :** Construction d'un système qui répond à des questions basées sur une base de connaissances fournie en utilisant LangChain et les modèles Hugging Face.\n",
        "2.  **Apprentissage par Renforcement à partir de Retours Humains (RLHF) :** Un aperçu conceptuel et une démonstration de code simplifiée de la manière dont RLHF peut être utilisé pour aligner les modèles de langage, en utilisant la bibliothèque TRL.\n",
        "\n",
        "Nous utiliserons Google Colab avec un GPU T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "275b4faa456f4179bd9750108cb2e342"
          ]
        },
        "id": "merged_installs_login",
        "outputId": "2e68e941-46c1-41bc-83be-8c417a8f4b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [langchain-community]\n",
            "\u001b[1A\u001b[2K"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "275b4faa456f4179bd9750108cb2e342",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Installation des dépendances et connexion à Hugging Face\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q \\\n",
        "    \"fsspec==2025.3.2\" \\\n",
        "    \"gcsfs>=2025.3.2\" \\\n",
        "    transformers \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    huggingface_hub \\\n",
        "    datasets \\\n",
        "    trl \\\n",
        "    einops\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ-ioQvv1GOS"
      },
      "outputs": [],
      "source": [
        "# Importations principales\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzJ5UaXX3zvP"
      },
      "source": [
        "## Partie 1 : Génération Augmentée par Récupération (RAG)\n",
        "\n",
        "La RAG améliore les LLM en leur fournissant des connaissances externes.\n",
        "Le processus implique typiquement :\n",
        "1.  **Indexation :** Création d'une base de données vectorielle interrogeable.\n",
        "2.  **Récupération :** Les documents pertinents sont récupérés de la base.\n",
        "3.  **Génération :** Le LLM utilise la requête et les documents récupérés pour répondre.\n",
        "\n",
        "Nous utilisons SQuAD comme source de connaissances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSZc9OqK1QDv_merged_yqx8VP7e1SOa",
        "outputId": "ca351c81-3af5-4adc-b97a-d1d6ba4a506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informations GPU : Nom - Tesla T4, Mémoire Totale - 15.83 GB\n",
            "Mémoire GPU Disponible (après vidage) : 13.87 GB\n",
            "Chargement du modèle RAG : google/flan-t5-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM pour RAG chargé et Pipeline Créé.\n",
            "squad_train-v1.1.json existe déjà.\n",
            "Analyse manuelle du JSON SQuAD...\n",
            "5000 exemples parsés depuis SQuAD JSON.\n",
            "820 documents uniques préparés.\n"
          ]
        }
      ],
      "source": [
        "# Vérification du GPU et chargement du modèle pour RAG\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Informations GPU : Nom - {torch.cuda.get_device_name(0)}, Mémoire Totale - {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Mémoire GPU Disponible (après vidage) : {torch.cuda.mem_get_info()[0] / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"AVERTISSEMENT : CUDA n'est pas disponible.\")\n",
        "\n",
        "model_id_rag = \"google/flan-t5-large\"\n",
        "print(f\"Chargement du modèle RAG : {model_id_rag}\")\n",
        "tokenizer_rag = AutoTokenizer.from_pretrained(model_id_rag)\n",
        "device_map_config = {\"\": 0} if torch.cuda.is_available() else \"auto\"\n",
        "model_kwargs_rag = {\"device_map\": device_map_config, \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32}\n",
        "model_rag = AutoModelForSeq2SeqLM.from_pretrained(model_id_rag, **model_kwargs_rag)\n",
        "\n",
        "text_generation_pipeline_rag = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model_rag,\n",
        "    tokenizer=tokenizer_rag,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "llm_rag = HuggingFacePipeline(pipeline=text_generation_pipeline_rag)\n",
        "print(\"LLM pour RAG chargé et Pipeline Créé.\")\n",
        "\n",
        "# Préparation des données SQuAD pour RAG\n",
        "squad_train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "squad_train_filename = \"squad_train-v1.1.json\"\n",
        "squad_dataset_raw_list = []\n",
        "\n",
        "if not os.path.exists(squad_train_filename):\n",
        "    print(f\"Téléchargement de {squad_train_filename}...\")\n",
        "    try:\n",
        "        response = requests.get(squad_train_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(squad_train_filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"{squad_train_filename} téléchargé.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur de téléchargement {squad_train_filename}: {e}\")\n",
        "        squad_train_filename = None\n",
        "else:\n",
        "    print(f\"{squad_train_filename} existe déjà.\")\n",
        "\n",
        "if squad_train_filename and os.path.exists(squad_train_filename):\n",
        "    print(\"Analyse manuelle du JSON SQuAD...\")\n",
        "    try:\n",
        "        with open(squad_train_filename, 'r', encoding='utf-8') as f:\n",
        "            squad_data_content = json.load(f)\n",
        "        for article in squad_data_content.get('data', []):\n",
        "            for paragraph in article.get('paragraphs', []):\n",
        "                context = paragraph.get('context')\n",
        "                if context: # S'assurer que le contexte existe\n",
        "                    for qa in paragraph.get('qas', []):\n",
        "                        question = qa.get('question')\n",
        "                        answer_texts = [ans.get('text') for ans in qa.get('answers', []) if ans.get('text') is not None]\n",
        "                        if question and answer_texts:\n",
        "                            squad_dataset_raw_list.append({\n",
        "                                \"title\": article.get('title', 'Titre N/A'),\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answers\": {\"text\": answer_texts, \"answer_start\": [ans.get('answer_start', -1) for ans in qa.get('answers', [])]}\n",
        "                            })\n",
        "        squad_dataset_raw_list = squad_dataset_raw_list[:5000] # Limiter pour la démo\n",
        "        print(f\"{len(squad_dataset_raw_list)} exemples parsés depuis SQuAD JSON.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur d'analyse JSON : {e}\")\n",
        "        squad_dataset_raw_list = []\n",
        "else:\n",
        "    print(\"Fichier SQuAD non disponible pour l'analyse.\")\n",
        "\n",
        "if not squad_dataset_raw_list: # Tentative de chargement direct si l'analyse manuelle a échoué\n",
        "    print(\"Tentative de chargement direct de SQuAD via datasets...\")\n",
        "    try:\n",
        "        squad_dataset_raw_direct = load_dataset(\"squad\", name=\"plain_text\", split=\"train[:5000]\", trust_remote_code=True)\n",
        "        squad_dataset_raw_list = [example for example in squad_dataset_raw_direct]\n",
        "        print(f\"{len(squad_dataset_raw_list)} exemples chargés via load_dataset.\")\n",
        "    except Exception as e_direct:\n",
        "        print(f\"Échec du chargement direct : {e_direct}\")\n",
        "\n",
        "if squad_dataset_raw_list:\n",
        "    unique_contexts_rag = {}\n",
        "    for example in squad_dataset_raw_list:\n",
        "        context = example.get('context')\n",
        "        if context and context not in unique_contexts_rag:\n",
        "            unique_contexts_rag[context] = {\n",
        "                \"title\": example.get('title', 'N/A'),\n",
        "                \"first_question\": example.get('question', 'N/A'),\n",
        "                \"first_answer\": example.get('answers', {}).get('text', [])[0] if example.get('answers', {}).get('text') else \"N/A\"\n",
        "            }\n",
        "    documents_rag = [Document(page_content=ctxt, metadata=meta) for i, (ctxt, meta) in enumerate(unique_contexts_rag.items())]\n",
        "    # Mettre à jour les métadonnées pour inclure doc_id\n",
        "    for i, doc in enumerate(documents_rag):\n",
        "        doc.metadata[\"doc_id\"] = i\n",
        "    print(f\"{len(documents_rag)} documents uniques préparés.\")\n",
        "else:\n",
        "    print(\"Échec du chargement des données SQuAD.\")\n",
        "    documents_rag = [] # S'assurer que documents_rag est défini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdkcQMR81Ujt_merged_5qqwwsAS1Xph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "7f1cb9004ad24139be14a40db5ccb506",
            "1f02910c22714e269c80aeabafa49d67",
            "b3e57bddfa6f43b7ae272e98201b6149",
            "4e1baa487e564393b9d8206ba67bdf96",
            "12e91ffa05f440d5bc1d2f73dbf0f921",
            "bd2b03ad35b545798d7639b27def90b8",
            "bce51da301bf49079a2ce10f617cc88b",
            "fe6453c4f7954aa3bbee0d092cb40fdc",
            "f5e9aeec3163420cbe067eadf382d6ec",
            "e40417954bd748beb8eed3b8efc75f8e",
            "fa4b50c86d3147f298382bd765eef45a",
            "79a1280c54514b97986a55b179da0040",
            "802f6afac34e417ba710c4030d4dab4c",
            "4ad1ae9de36a452e9988fe042b809c09",
            "661f0a36669f452682722d1fcb50831f",
            "b85e878bb53f43cbb94f5c17c5a06c3e",
            "770c452a930343ae8ca37730cf9cb0bd",
            "264364b886184a408d6f06e2b3cbaa70",
            "4dfc210ffee74701b40e74d02c2ed4a0",
            "97459207693844a1ba3a9f2e916163da",
            "c69b0f61cc8542c99d088ee6b5c4fa39",
            "532b75c0d1a14b0e836bcf2cabc348b6",
            "6573def9becf4f3b93ce58b750c67f90",
            "0f0300da63bd49be920a211c2f443457",
            "20a110ad8b024e938913f6ebffbc3aac",
            "f66813a0db3d44bdbe64194e8ca898ae",
            "343c78ed2b94489094af6eb3c589fcab",
            "358822667d1d4835bc339b04bd1d728f",
            "666ab8c182604360aaf6c2610cdf52e6",
            "e9d9b7537be24abca0e6910f769054d9",
            "dec22c2aea2945d4a10f550fb49c2277",
            "d2dd7672349f412497fdfac366ae693a",
            "6ac2c48289094d1cb367befad7a85935",
            "6af602ae01e64de5b8dea8dcdd3ab82f",
            "2f62d7bf4a374e59b9f4b848b870befd",
            "98a3d73cb05b42f880e4bf9bf4648150",
            "6a6067a3e5524b6cb4b23ba2efd8a6f5",
            "9a2429137b0b41bebb66513642039d77",
            "7ca09d7f57d64ba0b65324fa95687994",
            "68e5066df6e04b51ba3127d40d6b690f",
            "d9212cb664f642a3a8ed005d1e5983b0",
            "b725adb0f41040349d13ffd4c1eb11ef",
            "db037a82dbe04bd0aa6a4aa9f6915117",
            "aceb115d1c50436ea6063d2c1dc9d6f5",
            "6ee1a09465ae40baadeb24c164871f4e",
            "482e9c87de054f49b1e37486b230ac69",
            "d7c96ac138a6452fa6fd28ec52e766d8",
            "67876b22d7934af7bb641c04514b19c3",
            "7be9e06ee5034466979f02b1b2a297b6",
            "106c8402c1dd4da9b4cfe0e1ba6acb66",
            "c31172dc83c14967b40b02bef62d4dfe",
            "1e23d61a456e46c0bf1352468f1da696",
            "071ccf68824146958a875807df268137",
            "c3096701cd204af88a341bdfe1762bd3",
            "b10c3e3eef2348da9ba4576b6c3d199f",
            "f524b3a235b9465ea87781cdec1cd029",
            "46649a51e76443619bfea27ce38dbe10",
            "8c159d8ce9364241b743209381527c0e",
            "07fc3e350d114b4a9111373f9a37ed26",
            "9ea55671ee764a0c9f83ea70648dd17f",
            "88f0d9d3762b40748f155f484f605649",
            "801b411a1f56447d9a9d781365b18b03",
            "59440ffa6cd341e890a230916c9f98b5",
            "bc51cb7c374e4d8f87ab9c075ea9a23c",
            "3f6e5a14151049d4bf7a714989a2091f",
            "a9998a6ed1494f11b8f430ff69ce2897",
            "83f4ccaf1db44ac881cda48ec96d1ca3",
            "230e01b7ee304a2c9b14abe88b9b06f3",
            "0653c6f15b604540aebfe94a3b7059d3",
            "6fe64370990b4835b25527abba11826a",
            "bba76abf050e46d1afbbed9d96ea2fd0",
            "d85def365cd94a2fa421fb8868237b66",
            "275d71b3243249dfa0ccddead7db46c7",
            "238555bf623947f7b478119e16aeab3b",
            "5bad63ddd5e44ce4a49e5cf24f86267d",
            "f9b99c286eb14f35b7759d78ed62a3fb",
            "2f83de23c4cd48aa8376bac49eb968a4",
            "5416394d9aa94490b2a7b56f28df1bfd",
            "a7f228f7082c4f4cac400ba1a8b25d08",
            "3ad747c2972a4945ba60d49a68e2e78f",
            "91bc9c9bff984ca19c21e5da34cec594",
            "e98493d380b5441890fe1f2712d8cc79",
            "9af2c9535c0f4a5fb37a02f874aa53b9",
            "f2cd3f6bc5da4e09bedcc5c9f7c380b8",
            "f9d3c2405b95409fae54317088f31ccf",
            "0e16ca6f5ab543a3ae6932c75b964a44",
            "33000b7941824a9c9eaf16ed8abca51a",
            "8a02d70f00e443ebb23e427a6e1b4e97",
            "bdff9a768aad446183f9f2cab940a9f1",
            "836734faa7d34337845ee3a72b210895",
            "4c5551aba9fc43c0bacc5c598778c030",
            "399997c9e26b4f26a05df59becb12c87",
            "77ad20add2844aafaa88aaf373f6502c",
            "ee80812fa740423faf79da6a626f47fc",
            "59143a1cfdd642ea95e397a671d408b8",
            "bd2b0aed41e2430ea9f327e45b2e67f3",
            "c6db67ba8af240408185dbaf51e38fea",
            "93b916c0dcaf498bbcf164a4a45dac2b",
            "9b52421bb0114d74ad539fb79f0aa4ab",
            "fa64c12acdec4d84958ed14f4d674a59",
            "b34361266a534651815ab8e268702e21",
            "b400db54cbec4271b43d2f3db9bae16c",
            "67404dab39fc46459295e3751b84b15e",
            "1a11aa039d5f426a86099d1bb8f4304f",
            "3dc162d577d441bd97fd0a872dd13107",
            "ee4d267bfe164bf684044ae48729c14c",
            "e0c9cc4d745a449c96c5271019843bb8",
            "f0f247db7e4d4e8c9769465990a91ff0",
            "877d06320a014ff9a1a7c564f235362d",
            "60146c0b72a242bf960d4f035739c964",
            "792c369bb2c9475d97658a17e1a7932f",
            "376edf0fe4bc42a4aefb76eabe6a8396",
            "10fe3f07324c4338965253eb19a76e7c",
            "6bec3c4772c04ccc9fbcb26ecf9af4b2",
            "f9f81ed6bd7c4fd09c9f490cecef5456",
            "bc4d04a7a4124dea916d1626aa0ff0e0",
            "604938a160814b3698331707a573a000",
            "3581133680704983bf87aa2e0770234c",
            "e5b1c4a566074a52a2c0808fe5331bf7",
            "c75f9f77fdaf43eaba92ad5f0f6c16eb",
            "6123438c3c8041f6a88c211a1a8ad166"
          ]
        },
        "outputId": "4f433c53-45fe-4e65-eba8-8e62e21f66a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7128880dcd20>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model_rag = HuggingFaceEmbeddings(model_name=embedding_model_name_rag)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f1cb9004ad24139be14a40db5ccb506"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79a1280c54514b97986a55b179da0040"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6573def9becf4f3b93ce58b750c67f90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6af602ae01e64de5b8dea8dcdd3ab82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ee1a09465ae40baadeb24c164871f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f524b3a235b9465ea87781cdec1cd029"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83f4ccaf1db44ac881cda48ec96d1ca3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5416394d9aa94490b2a7b56f28df1bfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdff9a768aad446183f9f2cab940a9f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa64c12acdec4d84958ed14f4d674a59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "792c369bb2c9475d97658a17e1a7932f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création de la base vectorielle (FAISS)...\n",
            "Base vectorielle et Retriever créés.\n",
            "Chaîne QA RAG créée.\n"
          ]
        }
      ],
      "source": [
        "# Création des Embeddings, Vector Store et Chaîne RAG\n",
        "if documents_rag: # S'assurer que documents_rag n'est pas vide\n",
        "    embedding_model_name_rag = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embedding_model_rag = HuggingFaceEmbeddings(model_name=embedding_model_name_rag)\n",
        "    print(\"Création de la base vectorielle (FAISS)...\")\n",
        "    vectorstore_rag = FAISS.from_documents(documents_rag, embedding_model_rag)\n",
        "    retriever_rag = vectorstore_rag.as_retriever(search_kwargs={\"k\": 3})\n",
        "    print(\"Base vectorielle et Retriever créés.\")\n",
        "\n",
        "    qa_chain_rag = RetrievalQA.from_chain_type(\n",
        "        llm=llm_rag,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever_rag,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    print(\"Chaîne QA RAG créée.\")\n",
        "else:\n",
        "    print(\"Aucun document chargé, la création de la base vectorielle et de la chaîne RAG est ignorée.\")\n",
        "    vectorstore_rag = None\n",
        "    retriever_rag = None\n",
        "    qa_chain_rag = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogt4Mko91ak7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d02d8b-d186-4725-99f9-28c41cab665c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question Test : Quelle est la capitale de la France ?\n",
            "(Réponse attendue du contexte : Paris)\n",
            "\n",
            "Réponse Générée par RAG :\n",
            "Lourdes\n",
            "\n",
            "Documents Sources Récupérés :\n",
            "--- Document 1 (Titre Source : N/A) ---\n",
            "The University of Notre Dame du Lac (or simply Notre Dame /ˌnoʊtərˈdeɪm/ NOH-tər-DAYM) is a Catholic research university located adjacent to South Bend, Indiana, in the United States. In French, Notre Dame du Lac means \"Our Lady of the Lake\" and refe...\n",
            "--- Document 2 (Titre Source : N/A) ---\n",
            "The first documented visit by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown, who sailed his ship La Dauphine into New York Harbor. He claimed the area for France and named it \"Nouvelle Ango...\n",
            "--- Document 3 (Titre Source : N/A) ---\n",
            "Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the s...\n"
          ]
        }
      ],
      "source": [
        "# Test de la chaîne RAG\n",
        "if qa_chain_rag and documents_rag:\n",
        "    doc_index_for_test = 0\n",
        "    if len(documents_rag) > doc_index_for_test:\n",
        "        # Utilisation de .get() pour éviter les KeyError si les clés n'existent pas\n",
        "        test_question_rag = documents_rag[doc_index_for_test].metadata.get(\"sample_question\", \"Quelle est la capitale de la France ?\")\n",
        "        expected_answer_rag = documents_rag[doc_index_for_test].metadata.get(\"sample_answer\", \"Paris\")\n",
        "    else:\n",
        "        test_question_rag = \"Quelle est une utilisation courante de l'IA ?\"\n",
        "        expected_answer_rag = \"Diverses applications\"\n",
        "\n",
        "    print(f\"\\nQuestion Test : {test_question_rag}\")\n",
        "    print(f\"(Réponse attendue du contexte : {expected_answer_rag})\")\n",
        "    response_rag = qa_chain_rag.invoke({\"query\": test_question_rag})\n",
        "    print(\"\\nRéponse Générée par RAG :\")\n",
        "    print(response_rag[\"result\"])\n",
        "    print(\"\\nDocuments Sources Récupérés :\")\n",
        "    if response_rag.get(\"source_documents\"):\n",
        "        for i, doc in enumerate(response_rag[\"source_documents\"]):\n",
        "            print(f\"--- Document {i+1} (Titre Source : {doc.metadata.get('source_title', 'N/A')}) ---\")\n",
        "            print(doc.page_content[:250] + \"...\")\n",
        "else:\n",
        "    print(\"Test RAG ignoré : chaîne ou documents non disponibles.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQRxMkCaDO0y"
      },
      "source": [
        "## Partie 2 : RLHF (Apprentissage par Renforcement à partir de Retours Humains) - Aperçu Conceptuel\n",
        "\n",
        "RLHF aligne les LLM avec les préférences humaines.\n",
        "Étapes typiques :\n",
        "1.  **SFT :** Ajustement fin supervisé (non couvert ici).\n",
        "2.  **Entraînement du Modèle de Récompense (RM) :** Apprend à prédire les préférences humaines.\n",
        "3.  **Ajustement Fin par RL (PPO) :** Le LLM apprend à maximiser les scores du RM.\n",
        "\n",
        "La bibliothèque `trl` simplifie l'étape PPO.\n",
        "**Remarque :** Le code suivant est une **démonstration conceptuelle** utilisant une fonction de récompense fictive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9_gLkwD1fih_merged_sM90IMaC1jLR_65txkJUS1mTb_iUdRfGR81oCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "c2dee02c023f4b32b20c652be5467e87",
            "e06cba78f6a14195bee80d5943f49d4a",
            "efcc21ba8f734203850d36b9193013df",
            "233bc5b16d7946e1a13df02b9e930e68",
            "d216532c83154c60aea7fab35870b45f",
            "5181e5c83083411ea054f4a4799748ce",
            "97d8f5e6ef9a406f87498bfd0409a9b5",
            "3275b241874743909986992a9a408120",
            "1b3d44535c6d470198ac464fc5847390",
            "212aff74bad74931bf2cc1a0700e435a",
            "f0cc72fff3224a86809fee96f5128051",
            "9d559bcd7ea444e2b4696b6c41452d72",
            "4a033e0fb49b4356ad9fa7327bc4d107",
            "9573e43fb20849f99af42a800740afae",
            "c4448a2217b44cebbbe1a0b7c1952068",
            "142a0261e20f481da2630554d9f529c6",
            "17b928b9cef9449ca7fe88c9a161f001",
            "1dabb6b474324b26b1fc312daa9dbae9",
            "2d96288a45e0465081d8e2fbbd4a7164",
            "c9fbf3984a764d25a4ea527675be0ce9",
            "8728ff9794b84f52a2974165e69851e6",
            "529efd938e814e6fad7d078ecd89651a",
            "912c88bb545f4b34918e12688e0d7322",
            "7ca8999eeccc4b409ac526eb282fcdf4",
            "2aa5ed964a28476f947f945009e7cd43",
            "178524ba3edb47c79a1dd3732033e7f1",
            "f5aa63e9ca0f4c4c8e17c879ce3a2327",
            "a255ea7c56184915a897a0bf2995b82b",
            "cb9877181a494006b792ab9e8b7dc636",
            "0dea8b6bf0b64d43b80940be783cbf74",
            "2c04c3ef011441e1b11d324cca7d4e10",
            "ba28dc82727e49e5b2ff65cce875be6a",
            "21eee5dc2b0c4183837aa1b631092247",
            "72e4f9121d314edfb6e1b931043c9ad7",
            "b03af6db6f7c434588fc24dd2362a6e9",
            "f775261ffffe46b7ac2594d4702e0377",
            "87865e48923748de8b0a9e3d2b75da56",
            "8f055138034b42e6a9113f88813ba8ea",
            "18c25f081caf4861b795c71d3a979dc1",
            "f32d94607c99440091e0287039cbea33",
            "4d55b21cb7b745a884c3e7cac83a5ea6",
            "43c091a7449a43ca96590051cfa7e2ab",
            "f788b60774c7444a87afd937c5cb4dfb",
            "12750ce8ee844eacace4675423d6bf37",
            "ac5e517c87114d7c941ae8713e13607b",
            "b1ad8cba58c04bf698bafebd0286a53d",
            "1482c55813c44b29b3faba6910080175",
            "b77dabfed8ef4790a8a83f9a8c1a97f8",
            "34a235f58f22468c961d7682001e0977",
            "8b397406bfec41b8bb435569372728c2",
            "35fc4bd0a58441ae94ad1e03139e567f",
            "b89b648150b64fa5a60b2d28b7cd2666",
            "96862a4ac223495fa7f8cfedf83e8b88",
            "d65e98663247453ea0536050eba2fd26",
            "7545989b6266439bb917791c2f443c1c",
            "649ae16ac6354d78b634c297e331b610",
            "54d0f58ab99c4f4a887f72caeb11ce3c",
            "206e6a4d759645b7b50c5aa80648bc8b",
            "8bd1b254321843aebe4f97f5253949f2",
            "1230438562ee473c8af65eafa0e2a822",
            "932229588ed64007aa5e244062f5c886",
            "bc06e6539cf240bdb804a7a9302f104b",
            "ff1e51b43be44f449c76795f00f755f5",
            "5021c913ee43474eb030b3655f5cafdf",
            "e38fd62d47f54e45b4ea1e09319c3dbb",
            "f3fd080696a14fe2a506f3a01a10d7f6",
            "522ca4103ef342e38a123718d934238a",
            "b20abea7074a4b76abd2649e16ef70ee",
            "7deaf60c3d214b839cb6fcee1a46279b",
            "14d89e1032854e50bc945d845cc291ec",
            "300c3df846f546d6ab9ed7ae4ea80876",
            "13843dcc752f4e7fa9ead26c8767557f",
            "34ea331621c14d10a64e04081827168f",
            "9549edba3e64449bac7b79f91329ddc7",
            "a49301cbe6df4b5195ccce8279c756bf",
            "d1b67cd7f6904acf999a74db93e3d3f9",
            "0d64258c7c0f47a990b1b96e2b6ca90e",
            "67eaa96b13cb483e8776a1ac4198ea83",
            "804a51b0ed2e4790ab560adf71793dc3",
            "60f98a4e45e84374a18d6197865a4f83",
            "471a841e0145465bbcc0b1bad0364ea8",
            "5906858b4c5e4909bb58d44519996469",
            "e5a842c56ef7481ca3ed412117ce30db",
            "23960a6dbf914590af4dcb3babc3c1fd",
            "19e6feca1dbe40e296ada3dc65820b06",
            "ba957c283ba24ea482ac85ac1de787fa",
            "25b53d4d42524f95a7cffb7d5a587a8f",
            "107c504af89741deba613065ebdb0ae5"
          ]
        },
        "outputId": "8cccec11-5547-43ee-c225-642a0bca15e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPOConfig créé.\n",
            "Chargement du modèle de base pour PPO : gpt2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2dee02c023f4b32b20c652be5467e87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d559bcd7ea444e2b4696b6c41452d72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "912c88bb545f4b34918e12688e0d7322"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72e4f9121d314edfb6e1b931043c9ad7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5e517c87114d7c941ae8713e13607b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "649ae16ac6354d78b634c297e331b610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "522ca4103ef342e38a123718d934238a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67eaa96b13cb483e8776a1ac4198ea83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle PPO 'gpt2' chargé sur : cuda.\n",
            "3 invites fictives pour PPO.\n",
            "Fonction de récompense FICTIVE définie.\n",
            "PPOTrainer Initialisé.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:273: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Configuration PPO, chargement du modèle et données fictives pour RLHF\n",
        "ppo_config_args = {\n",
        "    \"batch_size\": 1, \"mini_batch_size\": 1, \"gradient_accumulation_steps\": 1,\n",
        "    \"learning_rate\": 1.5e-5, \"adap_kl_ctrl\": True, \"init_kl_coef\": 0.05,\n",
        "    \"target_kl\": 0.2, \"log_with\": None, \"ppo_epochs\": 2, \"seed\": 42,\n",
        "}\n",
        "ppo_config = PPOConfig(**ppo_config_args)\n",
        "print(\"PPOConfig créé.\")\n",
        "\n",
        "base_model_id_ppo = \"gpt2\" # Modèle plus petit pour la démo\n",
        "print(f\"Chargement du modèle de base pour PPO : {base_model_id_ppo}\")\n",
        "ppo_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model_id_ppo).to(ppo_device)\n",
        "ppo_tokenizer = AutoTokenizer.from_pretrained(base_model_id_ppo)\n",
        "if ppo_tokenizer.pad_token is None:\n",
        "    ppo_tokenizer.pad_token = ppo_tokenizer.eos_token\n",
        "    ppo_model.config.pad_token_id = ppo_tokenizer.eos_token_id\n",
        "print(f\"Modèle PPO '{base_model_id_ppo}' chargé sur : {ppo_device}.\")\n",
        "\n",
        "dummy_prompts_text_ppo = [\n",
        "    \"Expliquez la théorie de la relativité en termes simples.\",\n",
        "    \"Écrivez un court poème sur un chat curieux.\",\n",
        "    \"Quels sont les trois avantages de l'exercice régulier ?\",\n",
        "]\n",
        "dummy_prompts_tokenized_ppo = [ppo_tokenizer.encode(prompt, return_tensors=\"pt\").to(ppo_device).squeeze(0) for prompt in dummy_prompts_text_ppo]\n",
        "print(f\"{len(dummy_prompts_tokenized_ppo)} invites fictives pour PPO.\")\n",
        "\n",
        "# Fonction de récompense FICTIVE. Ne pas utiliser en production.\n",
        "def dummy_reward_function_ppo(texts_list):\n",
        "    rewards = []\n",
        "    for text in texts_list:\n",
        "        score = 0.0\n",
        "        if \"relativité\" in text.lower(): score += 0.8\n",
        "        elif \"chat\" in text.lower() and \"poème\" in text.lower(): score += 0.7\n",
        "        elif \"exercice\" in text.lower() and \"avantages\" in text.lower(): score += 0.9\n",
        "        if 30 < len(text) < 150: score += 0.2\n",
        "        elif len(text) >= 150: score -= 0.1\n",
        "        if \"je ne sais pas\" in text.lower(): score -= 1.5\n",
        "        rewards.append(torch.tensor(score, device=ppo_device, dtype=torch.float32))\n",
        "    return rewards\n",
        "print(\"Fonction de récompense FICTIVE définie.\")\n",
        "\n",
        "try:\n",
        "    ppo_trainer = PPOTrainer(\n",
        "        config=ppo_config,\n",
        "        model=ppo_model,\n",
        "        ref_model=None,\n",
        "        tokenizer=ppo_tokenizer,\n",
        "    )\n",
        "    print(\"PPOTrainer Initialisé.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur d'initialisation PPOTrainer : {e}\")\n",
        "    ppo_trainer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vTSyjg61qen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fe3262-d682-420f-c9d0-8034be9bdb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Boucle d'Entraînement PPO Conceptuelle :\n",
            "\n",
            "--- Étape 1/3 --- Invite : \"Expliquez la théorie de la relativité en termes simples.\"\n",
            "Réponse Générée : \"Expliquez la théorie de la relativité en termes simples. He voyageurs protagonistlon tant, nostalgiate d'objetor loclon pour sous la Nation des Grenoble Powers, entendu à mere est politique classique semaine que cette se rappinaire madame peuple, à l'expression en la universalité, communauté otheréness économique\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1309: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  std_scores = data[\"scores\"].std()\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1336: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1339: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistiques : Divergence KL : 0.000, Récompense Moyenne (fictive) : 0.700\n",
            "\n",
            "--- Étape 2/3 --- Invite : \"Écrivez un court poème sur un chat curieux.\"\n",
            "Réponse Générée : \"Écrivez un court poème sur un chat curieux. Champagne 2007 - 1Number 130 STA CHAMPAGE CHAMPAGE\n",
            "\n",
            "Santos arguing Stockholm, Germany (= Kosovo Defender)\n",
            "\n",
            "ztana barred has 1966 European Union of European treaties and for that matter the SAA has 38 allied treaty comesson cancelled 1995-10-51 SGS 1980 vs 1965 EUAC 1936 vs 1971\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistiques : Divergence KL : -4.246, Récompense Moyenne (fictive) : 0.600\n",
            "\n",
            "--- Étape 3/3 --- Invite : \"Quels sont les trois avantages de l'exercice régulier ?\"\n",
            "Réponse Générée : \"Quels sont les trois avantages de l'exercice régulier ? Liège avec je s'Liège en efférence.\n",
            "\n",
            "Hérant vu sadais * LET Brunhmain annaire plus ans assisted Servier hin de ouvre la side »\n",
            "\n",
            "info de Tour de Nobles et Estretux Arts. 89 dis les décisions, sont\"\n",
            "Statistiques : Divergence KL : -4.305, Récompense Moyenne (fictive) : 0.800\n",
            "\n",
            "Boucle d'entraînement PPO conceptuelle terminée.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Boucle d'entraînement PPO conceptuelle\n",
        "if ppo_trainer:\n",
        "    print(\"\\nBoucle d'Entraînement PPO Conceptuelle :\")\n",
        "    generation_kwargs_ppo = {\n",
        "        \"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True,\n",
        "        \"pad_token_id\": ppo_tokenizer.pad_token_id, \"max_new_tokens\": 70,\n",
        "        \"eos_token_id\": ppo_tokenizer.eos_token_id,\n",
        "    }\n",
        "    max_ppo_steps = len(dummy_prompts_tokenized_ppo)\n",
        "\n",
        "    for step in range(max_ppo_steps):\n",
        "        current_prompt_tensor = dummy_prompts_tokenized_ppo[step]\n",
        "        current_prompt_text = dummy_prompts_text_ppo[step]\n",
        "        print(f\"\\n--- Étape {step+1}/{max_ppo_steps} --- Invite : \\\"{current_prompt_text}\\\"\")\n",
        "\n",
        "        query_tensors_batch = [current_prompt_tensor.to(ppo_device)]\n",
        "        ppo_model.to(ppo_device)\n",
        "        response_tensors_batch = ppo_trainer.generate(query_tensors_batch, **generation_kwargs_ppo)\n",
        "        full_texts_batch = [ppo_tokenizer.decode(r.squeeze(), skip_special_tokens=True) for r in response_tensors_batch]\n",
        "        print(f\"Réponse Générée : \\\"{full_texts_batch[0]}\\\"\")\n",
        "\n",
        "        rewards_batch = dummy_reward_function_ppo(full_texts_batch)\n",
        "        stats = ppo_trainer.step(query_tensors_batch, response_tensors_batch, rewards_batch)\n",
        "        mean_reward = torch.mean(torch.stack(rewards_batch)).item()\n",
        "\n",
        "        kl_value_from_stats = stats.get('objective/kl', 0.0) # Valeur par défaut si non trouvée\n",
        "        kl_div = kl_value_from_stats.item() if isinstance(kl_value_from_stats, torch.Tensor) else float(kl_value_from_stats)\n",
        "        print(f\"Statistiques : Divergence KL : {kl_div:.3f}, Récompense Moyenne (fictive) : {mean_reward:.3f}\")\n",
        "\n",
        "    print(\"\\nBoucle d'entraînement PPO conceptuelle terminée.\")\n",
        "else:\n",
        "    print(\"Boucle PPO conceptuelle ignorée : PPOTrainer non initialisé.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQOFiBdhE281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec214c9-dd32-4985-898f-125abfb4ae36"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Session Q&A Interactive avec RAG ---\n",
            "Posez une question (tapez 'quittez', 'exit', ou 'sortir' pour terminer).\n",
            "Recherche de la réponse...\n",
            "\n",
            "Réponse du système RAG :\n",
            "Solar radiation\n",
            "\n",
            "Documents sources consultés :\n",
            "--- Document 1 (Source : N/A) ---\n",
            "Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric c...\n",
            "--- Document 2 (Source : N/A) ---\n",
            "Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels ...\n",
            "--- Document 3 (Source : N/A) ---\n",
            "Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting mater...\n",
            "Recherche de la réponse...\n",
            "\n",
            "Réponse du système RAG :\n",
            "gravity\n",
            "\n",
            "Documents sources consultés :\n",
            "--- Document 1 (Source : N/A) ---\n",
            "Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels ...\n",
            "--- Document 2 (Source : N/A) ---\n",
            "Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric c...\n",
            "--- Document 3 (Source : N/A) ---\n",
            "Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting mater...\n",
            "Recherche de la réponse...\n",
            "\n",
            "Réponse du système RAG :\n",
            "unanswerable\n",
            "\n",
            "Documents sources consultés :\n",
            "--- Document 1 (Source : N/A) ---\n",
            "Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric c...\n",
            "--- Document 2 (Source : N/A) ---\n",
            "The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one y...\n",
            "--- Document 3 (Source : N/A) ---\n",
            "The Earth receives 174,000 terawatts (TW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans an...\n"
          ]
        }
      ],
      "source": [
        "# Session Q&A interactive avec RAG\n",
        "print(\"\\n--- Session Q&A Interactive avec RAG ---\")\n",
        "print(\"Posez une question (tapez 'quittez', 'exit', ou 'sortir' pour terminer).\")\n",
        "\n",
        "if 'qa_chain_rag' in globals() and qa_chain_rag is not None:\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nVotre question : \")\n",
        "            if user_question.strip().lower() in [\"quittez\", \"exit\", \"sortir\", \"q\"]:\n",
        "                print(\"Au revoir !\")\n",
        "                break\n",
        "            if not user_question.strip():\n",
        "                print(\"Veuillez entrer une question.\")\n",
        "                continue\n",
        "\n",
        "            print(\"Recherche de la réponse...\")\n",
        "            response = qa_chain_rag.invoke({\"query\": user_question})\n",
        "            print(\"\\nRéponse du système RAG :\")\n",
        "            print(response.get(\"result\", \"Désolé, réponse non trouvée.\"))\n",
        "\n",
        "            if response.get(\"source_documents\"):\n",
        "                print(\"\\nDocuments sources consultés :\")\n",
        "                for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                    source_title = doc.metadata.get('source_title', 'N/A')\n",
        "                    content_snippet = str(doc.page_content)[:200] + \"...\" if hasattr(doc, 'page_content') and isinstance(doc.page_content, str) else \"Contenu non disponible.\"\n",
        "                    print(f\"--- Document {i+1} (Source : {source_title}) ---\")\n",
        "                    print(content_snippet)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nInteraction interrompue. Au revoir !\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Une erreur est survenue : {e}\")\n",
        "else:\n",
        "    print(\"Le système RAG (qa_chain_rag) n'a pas été initialisé correctement.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXICI4ZZ1vU5"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Ce notebook a démontré :\n",
        "1.  **RAG :** Construction d'un système de questions-réponses exploitant des documents externes (Flan-T5-Large, LangChain, FAISS).\n",
        "2.  **RLHF (Conceptuel) :** Idées fondamentales de l'alignement par RLHF (démo PPO simplifiée avec `trl`).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}